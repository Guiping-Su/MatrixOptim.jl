
## Classical Methods of Calculus

### Unconstrained Optimization

According to appendix 3 in [@hillier2012introduction], the analysis for an unconstrained function of several variables $f(\mathbf{x}),$ where $\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right),$ is similar. Thus, a necessary condition for a solution $\mathbf{x}=\mathbf{x}^{*}$ to be either a minimum or a maximum is that:

$$ \frac{\partial f(\boldsymbol{x})}{\partial x_{j}}=0 \quad \text { at } \mathbf{x}=\mathbf{x}^{*}, \quad \text { for } j=1,2, \dots, n$$

After the critical points that satisfy this condition are identified, each such point is then classified as a local minimum or maximum if the function is strictly convex or strictly concave, respectively, within a neighborhood of the point. (Additional analysis is required if the function is neither.) The global minimum and maximum would be found by comparing the local minima and maxima and then checking the value of the function as some of the variables approach $-\infty$ or $+\infty$. However, if the function is known to be convex or concave, then a critical point must be a global minimum or a global maximum, respectively.

### Constrained Optimization with Equality Constraints

> From a practical computational viewpoint, the method of Lagrange multipliers is not a particularly powerful procedure. [@hillier2012introduction]

#### Method of Lagrange Multipliers {-}

According to appendix 3 in [@hillier2012introduction], the procedure begins by formulating the Lagrangian function:

$$ h(\mathbf{x}, \boldsymbol{\lambda})=f(\mathbf{x})-\sum_{i=1}^{m} \lambda_{i}\left[g_{i}(\mathbf{x})-b_{i}\right] $$

where the new variables $\boldsymbol{\lambda}=\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)$ are called Lagrange multipliers. Notice the key fact that for the feasible values of $\mathbf{x}$:

$$ g_{i}(\mathbf{x})-b_{i}=0, \quad \text { for all } i $$

so $h(\mathbf{x}, \mathbf{\lambda})=f(\boldsymbol{x}) .$ Therefore, it can be shown that if $(\mathbf{x}, \mathbf{\lambda}) = \left(\mathbf{x}^{*}, \mathbf{\lambda}^{*}\right)$ is a local or global minimum or maximum for the unconstrained function $h(\mathbf{x}, \boldsymbol{\lambda}),$ then $\mathbf{x}^{*}$ is a corresponding critical point for the original problem. As a result, the method now reduces to analyzing $h(\mathbf{x}, \mathbf{\lambda})$ by the procedure just described for unconstrained optimization. Thus, the $n+m$ partial derivatives would be set equal to zero:

$$ \begin{align}
\frac{\partial h}{\partial x_{j}} &= \frac{\partial f}{\partial x_{j}} - \sum_{i=1}^{m} \lambda_{i} \frac{\partial g_{i}}{\partial x_{j}} = 0 \quad \text{for  } j = 1, 2, \ldots, n \\
\frac{\partial h}{\partial \lambda_{i}} &= -g_{i}(\mathbf{x})+b_{i} = 0 \quad \text{for  } i = 1, 2, \ldots, m
\end{align} $$

and then the critical points would be obtained by solving these equations for $(\mathbf{x}, \boldsymbol{\lambda})$. Notice that the last $m$ equations are equivalent to the constraints in the original problem, so only feasible solutions are considered. After further analysis to identify the global minimum or maximum of $h(\cdot),$ the resulting value of $\mathbf{x}$ is then the desired solution to the original problem.
