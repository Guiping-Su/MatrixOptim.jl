---
editor_options:
  chunk_output_type: console
---

# Markov Decision Process (MDP) {#MDP}

Markov chain is a special kind of stochastic processes, which can be used to model lots of phenomena. Usually, to describe the system is not enough, because the managers or engineers may want to optimize the system performance. So Markov decision processes can be used to include the decisions to be made and formulate the optimization problem.



## Markov Chain

> A Markov process is a stochastic process with the property that, the probability of any particular future behavior of the process, when its current state is known exactly, is not altered by additional knowledge concerning its past behavior. [@pinsky2010introduction]

The stationary transition probability matrix (STPM) or Markov matrix (MM) can be used to describe the behavior of a Markov process.

> A Markov process is completely defined once its transition probability matrix and initial state (or, more generally, the probability distribution of the initial state) are specified. [@pinsky2010introduction]

> Suppose that a transition probability matrix on a finite number of states has the property that when raised to some power `k`, the `k`-step transition probability matrix has all of its elements strictly positive. Such a transition probability matrix, or the corresponding Markov chain, is called regular. The most important fact concerning a regular Markov chain is the existence of a limiting probability distribution, and this distribution is independent of the initial state. [@pinsky2010introduction]

> A transition probability matrix is called doubly stochastic if the columns sum to one as well as the rows. If the matrix is regular, then the unique limiting distribution is the uniform distribution. [@pinsky2010introduction]



## Queueing Theory



## Inventory Management
